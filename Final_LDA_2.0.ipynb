{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "# import pyLDAvis.gensim  # Author: don't skip this\n",
    "# pyLDAvis.gensim.prepare\n",
    "\n",
    "# I think i need another one:\n",
    "import pyLDAvis.gensim_models\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# TF.IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('dutch')\n",
    "stop_words.extend(['tenlastelegging', 'hof', 'althans', 'tenlastegelegd', 'naan', 'verklaring', 'verklaren', 'benadelen', 'naam', 'aangeefster', 'aangever', 'aangev', 'verbalisant', 'slachtoffer', 'rechtbank', 'uur', 'uren', 'weten', 'bestaan', 'waarheid', 'daarvoor', 'genaamd', 'maken', 'gaan', 'toverweging', 'aanzien', 'bewijs', 'feit', 'grond', 'staan', 'vaststellen', 'halen', 'vervolgens', 'nemen', 'aanhouden', 'bevinden', 'officier', 'justitie', 'overtuigen', 'bewijzen', 'maken', 'stellen', 'leggen', 'dienen', 'vrijspreken', 'daarnaast', 'bezigen', 'willen', 'gaan', 'vervolgens', 'raken', 'weten', 'proberen', 'echter', 'vraag', 'verdenken', 'vervatten', 'beslissing', 'hoger_beroep', 'verkort_vonni', 'geacht', 'instellen', 'ander', 'zien', 'toebehoren', 'hoeveelheid', 'lijst_ii', 'bereiken'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19966, 8)\n",
      "(520, 8)\n",
      "(334, 8)\n",
      "(2417, 8)\n",
      "(428, 8)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/dj/Python - UvA/DSP/data/df_final.csv', index_col=6, parse_dates=['date']) \n",
    "\n",
    "df_be = df[df['bewijs'] != '[]'] #bewijs\n",
    "df_te = df[df['tll'] != '[]'] #tenlastelegging\n",
    "\n",
    "df['bete'] = df['bewijs'] + df['tll'] \n",
    "df = df[df['bete'] != '[][]']\n",
    "df = df.drop(labels=['bewijs', 'tll'], axis=1)\n",
    "\n",
    "# ---- Crime type\n",
    "\n",
    "harddrugs_pattern = 'harddrugs|hard drugs'\n",
    "inbraak_pattern = 'inbraak|huisbraak|inbreken'\n",
    "mishandeling_pattern = 'mishandeling|mishandelen'\n",
    "verkrachting_pattern = 'verkrachting|verkrachten|verkracht'\n",
    "\n",
    "df2 = df.dropna(subset = ['inhoudsindicatie'])\n",
    "print(df2.shape)\n",
    "df_inbraak = df2[df2['inhoudsindicatie'].str.contains(inbraak_pattern)]\n",
    "print(df_inbraak.shape)\n",
    "df_harddrugs = df2[df2['inhoudsindicatie'].str.contains(harddrugs_pattern)]\n",
    "print(df_harddrugs.shape)\n",
    "df_mishandeling = df2[df2['inhoudsindicatie'].str.contains(mishandeling_pattern)]\n",
    "print(df_mishandeling.shape)\n",
    "df_verkrachting = df2[df2['inhoudsindicatie'].str.contains(verkrachting_pattern)]\n",
    "print(df_verkrachting.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>ecli</th>\n",
       "      <th>subject</th>\n",
       "      <th>spatial</th>\n",
       "      <th>case_nr</th>\n",
       "      <th>inhoudsindicatie</th>\n",
       "      <th>bete</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-17</th>\n",
       "      <td>9</td>\n",
       "      <td>ECLI_NL_GHAMS_2018_104.xml</td>\n",
       "      <td>ECLI:NL:GHAMS:2018:104</td>\n",
       "      <td>Strafrecht</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>23-001099-16.a</td>\n",
       "      <td>\\nVrijspraak wegens onvoldoende overtuigend be...</td>\n",
       "      <td>[][' tenlastelegging   aan de verdachte is ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-17</th>\n",
       "      <td>10</td>\n",
       "      <td>ECLI_NL_GHAMS_2018_105.xml</td>\n",
       "      <td>ECLI:NL:GHAMS:2018:105</td>\n",
       "      <td>Strafrecht</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>23-002194-17.a</td>\n",
       "      <td>\\nVeroordeling wegens winkeldiefstal met recid...</td>\n",
       "      <td>[][' tenlastelegging   aan de verdachte is ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-30</th>\n",
       "      <td>16</td>\n",
       "      <td>ECLI_NL_GHAMS_2018_1064.xml</td>\n",
       "      <td>ECLI:NL:GHAMS:2018:1064</td>\n",
       "      <td>Strafrecht</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>23-002597-17</td>\n",
       "      <td>\\nbelediging\\n</td>\n",
       "      <td>[][' tenlastelegging   aan de verdachte is ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-30</th>\n",
       "      <td>17</td>\n",
       "      <td>ECLI_NL_GHAMS_2018_1065.xml</td>\n",
       "      <td>ECLI:NL:GHAMS:2018:1065</td>\n",
       "      <td>Strafrecht</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>23-002801-16</td>\n",
       "      <td>\\ndiefstal\\n</td>\n",
       "      <td>[][' tenlastelegging   aan de verdachte is ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-30</th>\n",
       "      <td>19</td>\n",
       "      <td>ECLI_NL_GHAMS_2018_1072.xml</td>\n",
       "      <td>ECLI:NL:GHAMS:2018:1072</td>\n",
       "      <td>Strafrecht</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>23-002206-17.a</td>\n",
       "      <td>\\nDe verdachte wordt veroordeeld wegens het be...</td>\n",
       "      <td>['  bewijsmiddelen   nu de verdachte ter terec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Unnamed: 0                     filename                     ecli  \\\n",
       "date                                                                           \n",
       "2018-01-17           9   ECLI_NL_GHAMS_2018_104.xml   ECLI:NL:GHAMS:2018:104   \n",
       "2018-01-17          10   ECLI_NL_GHAMS_2018_105.xml   ECLI:NL:GHAMS:2018:105   \n",
       "2018-03-30          16  ECLI_NL_GHAMS_2018_1064.xml  ECLI:NL:GHAMS:2018:1064   \n",
       "2018-03-30          17  ECLI_NL_GHAMS_2018_1065.xml  ECLI:NL:GHAMS:2018:1065   \n",
       "2018-03-30          19  ECLI_NL_GHAMS_2018_1072.xml  ECLI:NL:GHAMS:2018:1072   \n",
       "\n",
       "               subject    spatial         case_nr  \\\n",
       "date                                                \n",
       "2018-01-17  Strafrecht  Amsterdam  23-001099-16.a   \n",
       "2018-01-17  Strafrecht  Amsterdam  23-002194-17.a   \n",
       "2018-03-30  Strafrecht  Amsterdam    23-002597-17   \n",
       "2018-03-30  Strafrecht  Amsterdam    23-002801-16   \n",
       "2018-03-30  Strafrecht  Amsterdam  23-002206-17.a   \n",
       "\n",
       "                                             inhoudsindicatie  \\\n",
       "date                                                            \n",
       "2018-01-17  \\nVrijspraak wegens onvoldoende overtuigend be...   \n",
       "2018-01-17  \\nVeroordeling wegens winkeldiefstal met recid...   \n",
       "2018-03-30                                     \\nbelediging\\n   \n",
       "2018-03-30                                       \\ndiefstal\\n   \n",
       "2018-03-30  \\nDe verdachte wordt veroordeeld wegens het be...   \n",
       "\n",
       "                                                         bete  \n",
       "date                                                           \n",
       "2018-01-17  [][' tenlastelegging   aan de verdachte is ten...  \n",
       "2018-01-17  [][' tenlastelegging   aan de verdachte is ten...  \n",
       "2018-03-30  [][' tenlastelegging   aan de verdachte is ten...  \n",
       "2018-03-30  [][' tenlastelegging   aan de verdachte is ten...  \n",
       "2018-03-30  ['  bewijsmiddelen   nu de verdachte ter terec...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''''\n",
    "df = pd.read_csv('/Users/dj/Python - UvA/DSP/data/dataset_18_21.csv', index_col=5, parse_dates=['date']) \n",
    "df = df.drop(labels=['ecli', 'case_nr', 'bewijs'], axis=1)\n",
    "df = df.rename(columns={\"tll\": \"bete\"})\n",
    "df.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is deprecated and will raise a KeyError in a future Version.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is deprecated and will raise a KeyError in a future Version.\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is deprecated and will raise a KeyError in a future Version.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is deprecated and will raise a KeyError in a future Version.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "df18 = df.loc[\"2018-01-01 00:00:00\":\"2018-12-31 00:00:00\"]\n",
    "df19 = df.loc[\"2019-01-01 00:00:00\":\"2019-12-31 00:00:00\"]\n",
    "df20 = df.loc[\"2020-01-01 00:00:00\":\"2020-12-31 00:00:00\"]\n",
    "df21 = df.loc[\"2021-01-01 00:00:00\":\"2021-12-31 00:00:00\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5062, 8)\n",
      "(5076, 8)\n",
      "(4961, 8)\n",
      "(4877, 8)\n"
     ]
    }
   ],
   "source": [
    "print(df18.shape)\n",
    "print(df19.shape)\n",
    "print(df20.shape)\n",
    "print(df21.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Giant Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_lda_model(df):\n",
    "\n",
    "    def sent_to_words(sentences):\n",
    "        for sentence in sentences:\n",
    "            yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "    data_words = list(sent_to_words(df['bete']))\n",
    "\n",
    "        # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    nlp = spacy.load(\"nl_core_news_sm\")\n",
    "\n",
    "    def sent_to_words(sentences):\n",
    "        for sentence in sentences:\n",
    "            yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "    # Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "    def remove_stopwords(texts):\n",
    "        return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "    def make_bigrams(texts):\n",
    "        return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "    def make_trigrams(texts):\n",
    "        return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "    def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "        texts_out = []\n",
    "        for sent in texts:\n",
    "            doc = nlp(\" \".join(sent)) \n",
    "            texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "        return texts_out\n",
    "\n",
    "\n",
    "\n",
    "    # Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    # nlp = spacy.load('nl', disable=['parser', 'ner'])\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "    data_lemmatized = remove_stopwords(data_lemmatized)\n",
    "\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = data_lemmatized\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    # corpus1 = [str(item) for item in corpus]\n",
    "\n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=5, \n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto',\n",
    "                                            per_word_topics=True)\n",
    "\n",
    "\n",
    "    ######## TF.IDF ########\n",
    "\n",
    "    dat_lem_untok = [\" \".join(x) for x in data_lemmatized]\n",
    "    tfidf_vectorizer = TfidfVectorizer(input=dat_lem_untok, stop_words=stop_words)\n",
    "    tfidf_vector = tfidf_vectorizer.fit_transform(dat_lem_untok)\n",
    "    tfidf_df = pd.DataFrame(tfidf_vector.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "    tfidf_df = tfidf_df.stack().reset_index()\n",
    "    tfidf_df = tfidf_df.rename(columns={0:'tfidf', 'level_0': 'document','level_1': 'term', 'level_2': 'term'})\n",
    "    top_tfidf = tfidf_df.sort_values(by=['document','tfidf'], ascending=[True,False]).groupby(['document']).head(10)\n",
    "\n",
    "     \n",
    "\n",
    "    return(lda_model, data_lemmatized, corpus, id2word, top_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [df18, df19, df20, df21]\n",
    "q = 1\n",
    "lda_models = []\n",
    "dat_lems = []\n",
    "corps = []\n",
    "ids = []\n",
    "top_idfs = []\n",
    "\n",
    "for i in d:\n",
    "    print(q)\n",
    "    q += 1\n",
    "    l, dl, c, i2words, t = get_lda_model(i)\n",
    "    lda_models.append(l)\n",
    "    dat_lems.append(dl)\n",
    "    corps.append(c)\n",
    "    ids.append(i2words)\n",
    "    top_idfs.append(t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Model Perplexity and Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18 = 0 / 19 = 1 / 20 = 2 / 21 = 3\n",
    "\n",
    "a = 3\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_models[a].log_perplexity(corps[a]))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_models[a], texts=dat_lems[a], dictionary=ids[a], coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the topics-keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18 = 0 / 19 = 1 / 20 = 2 / 21 = 3\n",
    "\n",
    "a = 0\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_models[a], corps[a], ids[a], R = 10)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF.IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = top_idfs[0].sort_values(by=['tfidf'], ascending=[False]).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_idfs[0].sort_values(by=['tfidf'], ascending=[False]).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_idfs[3][top_idfs[3]['term'].str.contains('hof')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "years = ('2018', '2019', '2020', '2021')\n",
    "term = 'kledingwinkel'\n",
    "\n",
    "def avg_terms(data, term):\n",
    "    avg = data[data['term'].str.contains(term)]['tfidf'].mean()\n",
    "    return(avg)\n",
    "    \n",
    "for i in top_idfs:\n",
    "    if term in list(i['term']):\n",
    "        a  = avg_terms(i, term)\n",
    "        values.append(a)\n",
    "    else:\n",
    "        values.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pos = [0,1,2,3]\n",
    "\n",
    "figure(figsize=(12, 10), dpi=80)\n",
    "\n",
    "plt.plot(values)\n",
    "\n",
    "\n",
    "# Add title and axis names\n",
    "plt.title('Average TF-IDF Scores of term: {}'.format(term))\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('TF.IDF Scores')\n",
    " \n",
    "# Create names on the x axis\n",
    "plt.xticks(x_pos, years)\n",
    " \n",
    "# Show graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "\n",
    "# 18 = 0 / 19 = 1 / 20 = 2 / 21 = 3\n",
    "\n",
    "a = 0\n",
    "\n",
    "# Terms in this list will get a red dot in the visualization\n",
    "term_list = ['kledingwinkel', 'albert_heijn'] # Highlight the words of interest\n",
    "\n",
    "# adding a little randomness to break ties in term ranking\n",
    "top_tfidf_plusRand = top_idfs[a].copy()\n",
    "top_tfidf_plusRand = top_tfidf_plusRand.iloc[:500,]\n",
    "top_tfidf_plusRand['tfidf'] = top_tfidf_plusRand['tfidf'] + np.random.rand(top_tfidf_plusRand.shape[0])*0.0001\n",
    "\n",
    "# base for all visualizations, with rank calculation\n",
    "base = alt.Chart(top_tfidf_plusRand).encode(\n",
    "    x = 'rank:O',\n",
    "    y = 'document:N'\n",
    ").transform_window(\n",
    "    rank = \"rank()\",\n",
    "    sort = [alt.SortField(\"tfidf\", order=\"descending\")],\n",
    "    groupby = [\"document\"],\n",
    ")\n",
    "\n",
    "# heatmap specification\n",
    "heatmap = base.mark_rect().encode(\n",
    "    color = 'tfidf:Q'\n",
    ")\n",
    "\n",
    "# red circle over terms in above list\n",
    "circle = base.mark_circle(size=100).encode(\n",
    "    color = alt.condition(\n",
    "        alt.FieldOneOfPredicate(field='term', oneOf=term_list),\n",
    "        alt.value('red'),\n",
    "        alt.value('#FFFFFF00')        \n",
    "    )\n",
    ")\n",
    "\n",
    "# text labels, white for darker heatmap colors\n",
    "text = base.mark_text(baseline='middle').encode(\n",
    "    text = 'term:N',\n",
    "    color = alt.condition(alt.datum.tfidf >= 0.23, alt.value('white'), alt.value('black'))\n",
    ")\n",
    "\n",
    "# display the three superimposed visualizations\n",
    "(heatmap + circle + text).properties(width = 1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scattertext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, this plot does not work in visual studio code. One has to use the jupyter notebook in the safari browser. At least thats the case for me. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scattertext as st\n",
    "import re, io\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import rankdata, hmean, norm\n",
    "import spacy\n",
    "import os, pkgutil, json, urllib\n",
    "from urllib.request import urlopen\n",
    "from IPython.display import IFrame\n",
    "from IPython.core.display import display, HTML\n",
    "from scattertext import CorpusFromPandas, produce_scattertext_explorer\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scattertext as st\n",
    "import re, io\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import rankdata, hmean, norm\n",
    "import spacy\n",
    "import os, pkgutil, json, urllib\n",
    "from urllib.request import urlopen\n",
    "from IPython.display import IFrame\n",
    "from IPython.core.display import display, HTML\n",
    "from scattertext import CorpusFromPandas, produce_scattertext_explorer\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import webbrowser\n",
    "import codecs\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = d[0]\n",
    "d1 = d[1]\n",
    "\n",
    "d[0]['lemm'] = [\" \".join(x) for x in dat_lems[0]]\n",
    "d[1]['lemm'] = [\" \".join(x) for x in dat_lems[1]]\n",
    "\n",
    "d0['year'] = pd.to_datetime(d0.index).year\n",
    "d1['year'] = pd.to_datetime(d1.index).year\n",
    "\n",
    "d0['year'] = d0['year'].astype(\"string\")\n",
    "d1['year'] = d1['year'].astype(\"string\")\n",
    "\n",
    "frames = [d0, d1]\n",
    "\n",
    "result = pd.concat(frames)\n",
    "\n",
    "nlp = spacy.load(\"nl_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['parsed'] = result.lemm.apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.rename(columns={\"Unnamed: 0\": \"Doc_Number\"})\n",
    "result[\"Doc_Number\"] = result[\"Doc_Number\"].astype(\"category\")\n",
    "result[\"year\"] = result[\"year\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_scat_text = st.CorpusFromParsedDocuments(result, category_col='year', parsed_col='parsed').build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = produce_scattertext_explorer(corpus_scat_text,\n",
    "                                    category='2018',\n",
    "                                    category_name='2018',\n",
    "                                    not_category_name='2019',\n",
    "                                    width_in_pixels=1000,\n",
    "                                    minimum_term_frequency=5,\n",
    "                                    metadata=corpus_scat_text.get_df()['filename'],\n",
    "                                    transform= st.Scalers.log_scale_standardize) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import webbrowser\n",
    "import IPython\n",
    "\n",
    "file_name = 'result_scatter_scale.html'\n",
    "open(file_name, 'wb').write(html.encode('utf-8'))\n",
    "IFrame(src=file_name, width = 1200, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying so use data_lemmatized as input\n",
    "test = df.iloc[0:10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['test'] = test.bete.apply(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test['bete'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test['test'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dat_lems[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0]['lemm'] = dat_lems[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_lem_untok = [\" \".join(x) for x in dat_lems[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dat_lem_untok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(d[0]['lemm'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "d \n",
    "q \n",
    "lda_models \n",
    "dat_lems \n",
    "corps \n",
    "ids \n",
    "top_idfs \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_idfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/Users/dj/Python - UvA/DSP/data_2018.csv', index_col=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
